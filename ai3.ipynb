{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FinanceDataReader as fdr\n",
    "import datetime\n",
    "\n",
    "start_date = datetime.datetime.now() - datetime.timedelta(days=365*20)\n",
    "# 현재 시간으로부터 20년 전 날짜 계산\n",
    "\n",
    "\n",
    "df_kospi = fdr.DataReader('KS11', start_date).add_suffix('_kospi')\n",
    "df_kosdaq = fdr.DataReader('KQ11', start_date).add_suffix('_kosdaq')\n",
    "df_dow = fdr.DataReader('DJI', start_date).add_suffix('_dow')\n",
    "df_nasdaq = fdr.DataReader('IXIC', start_date).add_suffix('_nasdaq')\n",
    "df_snp500 = fdr.DataReader('US500', start_date).add_suffix('_s&p500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_kospi</th>\n",
       "      <th>High_kospi</th>\n",
       "      <th>Low_kospi</th>\n",
       "      <th>Close_kospi</th>\n",
       "      <th>Adj Close_kospi</th>\n",
       "      <th>Volume_kospi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-03-07</th>\n",
       "      <td>547.890015</td>\n",
       "      <td>555.380005</td>\n",
       "      <td>544.090027</td>\n",
       "      <td>546.020020</td>\n",
       "      <td>546.020020</td>\n",
       "      <td>635100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-03-10</th>\n",
       "      <td>545.539978</td>\n",
       "      <td>548.289978</td>\n",
       "      <td>541.289978</td>\n",
       "      <td>544.239990</td>\n",
       "      <td>544.239990</td>\n",
       "      <td>607100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-03-11</th>\n",
       "      <td>531.780029</td>\n",
       "      <td>538.619995</td>\n",
       "      <td>530.549988</td>\n",
       "      <td>532.530029</td>\n",
       "      <td>532.530029</td>\n",
       "      <td>657900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-03-12</th>\n",
       "      <td>531.169983</td>\n",
       "      <td>534.760010</td>\n",
       "      <td>524.530029</td>\n",
       "      <td>531.809998</td>\n",
       "      <td>531.809998</td>\n",
       "      <td>793800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-03-13</th>\n",
       "      <td>530.859985</td>\n",
       "      <td>531.780029</td>\n",
       "      <td>514.429993</td>\n",
       "      <td>531.780029</td>\n",
       "      <td>531.780029</td>\n",
       "      <td>679700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-22</th>\n",
       "      <td>2430.870117</td>\n",
       "      <td>2435.979980</td>\n",
       "      <td>2416.159912</td>\n",
       "      <td>2417.679932</td>\n",
       "      <td>2417.679932</td>\n",
       "      <td>426700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-23</th>\n",
       "      <td>2430.010010</td>\n",
       "      <td>2449.620117</td>\n",
       "      <td>2422.620117</td>\n",
       "      <td>2439.090088</td>\n",
       "      <td>2439.090088</td>\n",
       "      <td>400300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-24</th>\n",
       "      <td>2442.489990</td>\n",
       "      <td>2452.969971</td>\n",
       "      <td>2421.469971</td>\n",
       "      <td>2423.610107</td>\n",
       "      <td>2423.610107</td>\n",
       "      <td>369200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-27</th>\n",
       "      <td>2405.419922</td>\n",
       "      <td>2405.560059</td>\n",
       "      <td>2383.760010</td>\n",
       "      <td>2402.639893</td>\n",
       "      <td>2402.639893</td>\n",
       "      <td>356700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28</th>\n",
       "      <td>2417.580078</td>\n",
       "      <td>2431.669922</td>\n",
       "      <td>2407.290039</td>\n",
       "      <td>2412.850098</td>\n",
       "      <td>2412.850098</td>\n",
       "      <td>529200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4984 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open_kospi   High_kospi    Low_kospi  Close_kospi  \\\n",
       "Date                                                             \n",
       "2003-03-07   547.890015   555.380005   544.090027   546.020020   \n",
       "2003-03-10   545.539978   548.289978   541.289978   544.239990   \n",
       "2003-03-11   531.780029   538.619995   530.549988   532.530029   \n",
       "2003-03-12   531.169983   534.760010   524.530029   531.809998   \n",
       "2003-03-13   530.859985   531.780029   514.429993   531.780029   \n",
       "...                 ...          ...          ...          ...   \n",
       "2023-02-22  2430.870117  2435.979980  2416.159912  2417.679932   \n",
       "2023-02-23  2430.010010  2449.620117  2422.620117  2439.090088   \n",
       "2023-02-24  2442.489990  2452.969971  2421.469971  2423.610107   \n",
       "2023-02-27  2405.419922  2405.560059  2383.760010  2402.639893   \n",
       "2023-02-28  2417.580078  2431.669922  2407.290039  2412.850098   \n",
       "\n",
       "            Adj Close_kospi  Volume_kospi  \n",
       "Date                                       \n",
       "2003-03-07       546.020020      635100.0  \n",
       "2003-03-10       544.239990      607100.0  \n",
       "2003-03-11       532.530029      657900.0  \n",
       "2003-03-12       531.809998      793800.0  \n",
       "2003-03-13       531.780029      679700.0  \n",
       "...                     ...           ...  \n",
       "2023-02-22      2417.679932      426700.0  \n",
       "2023-02-23      2439.090088      400300.0  \n",
       "2023-02-24      2423.610107      369200.0  \n",
       "2023-02-27      2402.639893      356700.0  \n",
       "2023-02-28      2412.850098      529200.0  \n",
       "\n",
       "[4984 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kospi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heelim = fdr.DataReader('037440', start_date).add_suffix('_heelim')\n",
    "df_aps = fdr.DataReader('054620', start_date).add_suffix('_aps_holdings')\n",
    "data_aps = pd.merge(df_kospi, df_kosdaq, on='Date').merge(df_dow, on='Date').merge(df_nasdaq, on='Date').merge(df_snp500, on='Date').merge(df_aps, on='Date')\n",
    "data_heelim = pd.merge(df_kospi, df_kosdaq, on='Date').merge(df_dow, on='Date').merge(df_nasdaq, on='Date').merge(df_snp500, on='Date').merge(df_heelim, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_size = 784\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='dataset/', train=True,  transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False,  transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 4633808.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 7382721.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 3576523.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 12354428.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 784, got 28",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(scores, targets)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n",
      "File \u001b[0;32m~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1189'>1190</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1190'>1191</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1191'>1192</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1192'>1193</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1193'>1194</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1194'>1195</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1195'>1196</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[0;32m~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1189'>1190</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1190'>1191</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1191'>1192</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1192'>1193</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1193'>1194</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1194'>1195</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1195'>1196</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:472\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=468'>469</a>\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=470'>471</a>\u001b[0m \u001b[39massert\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=471'>472</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=472'>473</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_RELU\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=473'>474</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:234\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=232'>233</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=233'>234</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=234'>235</a>\u001b[0m     expected_hidden_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=236'>237</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:210\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=205'>206</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=206'>207</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=207'>208</a>\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=209'>210</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=210'>211</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///~/work.ai/finaince_2/myenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py?line=211'>212</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 784, got 28"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize network\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to coda if possible\n",
    "        data = data.to(device=device).squeeze(1)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on training data')\n",
    "    else:\n",
    "        print('checking accuracy on test data')\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "            print(f'Got {num_correct} / {num_samples} with accuracy \\\n",
    "                {float(num_correct) / float(num_samples) * 100:.2f}')\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23ee331adf06ea98f3e736413c48b43d56a7083e6acdfa5cbf92e43d8ef580c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
